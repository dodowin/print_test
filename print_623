#!/usr/bin/env python  
# author: MachineBaby
from kouzi_tool.mysql import ScrapyDB, XiunoDB
from pyquery import PyQuery as pq
from tool import rand_header, string_for_mysql
import requests
from requests import exceptions
import logging
import time
import random


class RongBot(object):

    def __init__(self):
        self.scrapydb = ScrapyDB()
        self.xiunodb = XiunoDB()

    @staticmethod
    def parse_urls(text):
        """
            # 解析函数
            # url fetch  status
            # error:-1 new : 0 success:1 update:2
            # 如果是 同一个链接 出现 内容更新 就比较麻烦，需要记录 时间
            # id type site url status timestamp
        """
        doc = pq(text)
        # return doc(".thread .subject_link").items()
        commons = doc("th").items()
        item_list = []
        for common in commons:
            imgs = common.find("img").items()
            digest = False
            for img in imgs:
                src = img.attr("src")
                if "digest" in src or "hot" in src:
                    digest = True
            if digest:
                link = common.find(".xst:first")
                title = link.text()
                href = link.attr("href")
                url = "https://bbs.rong360.com/" + href
                tid = href.split("-")[1]
                page = common.find(".tps a:last").text()
                item = {}
                # print(page)
                if page:
                    item["page"] = page
                else:
                    item["page"] = 1
                item["title"] = title
                item["url"] = url
                item["tid"] = tid
                item_list.append(item)
        return item_list

    @staticmethod
    def parse_posts(text):
        doc = pq(text)
        # title = doc("h1").text()
        doc(".quote").remove()

        plhins = doc(".plhin").items()
        # item['title'] = ''
        posts = []
        for i, plhin in enumerate(plhins):
            item = {}
            user_nick = plhin(".pls .authi a").text()

            post_time = plhin(".plc .authi em span").attr("title")
            if post_time is None:
                post_time = plhin(".plc .authi em").text()
                post_time = post_time.replace("发表于", "").strip()

            post = plhin(".t_f")
            post(".tip").remove()
            if i == 0:
                post("br").prepend("[br]")
                # 改变图片链接
                first_img = post(".zoom").attr.zoomfile
                if first_img:
                    img_src = "https://bbs.rong360.com/{}".format(first_img)
                    item["img_src"] = img_src
                    # img_url_tag = '[img src="https://bbs.rong360.com/{}"]'.format(first_img)
                    post("img:first").prepend("[img]")
                    # img_html = '<p><img src="%s" title="%s" alt="%s"/></p>' % (img_url, img_do[1], img_do[1])

                post = plhin(".t_f").text()
            else:
                # item['top'] = 0
                post = post.text()
                post = string_for_mysql(post)
            item['post'] = post
            item['user'] = user_nick
            item['time'] = post_time
            # print(item)
            if len(post) > 3:
                posts.append(item)
                # yield item
        return posts

    @staticmethod
    def fetch(url, rand_head=False):
        header = rand_header(rand_head)
        # 抓取函数
        # 返回元组 (状态,页面内容)
        try:
            response = requests.get(url, headers=header, timeout=(3.05, 10))
        except exceptions.HTTPError as e:
            print('http请求错误:' + str(e.message))
        else:
            print(response.status_code)
        if response.history:
            logging.debug("redirect: url=%s" % url)
        if response.status_code == 200:
            return True, response.text
        else:
            return False, response.status_code

    def check_tid_in_mysql(self, tid):  # 查询mysql 判断重复url
        sql = "select 1 from `bot_urls`  where tid ={} limit 1;".format(tid)
        return self.xiunodb.db_execute(sql)

    def save_urls(self, items):
        # 存储url 列表
        for item in items:
            is_old_url = self.check_tid_in_mysql(int(item["tid"]))

            if is_old_url:        #update page
                sql = "UPDATE `bot_urls` SET count_page={} WHERE tid={};".format(item['page'], item['tid'])
            else:
                sql = "INSERT INTO `bot_urls` (`tid`, `url`, `title`,`count_page`,`site`)" \
                      " VALUES ('{}', '{}','{}','{}','{}');"\
                    .format(item['tid'], item['url'], item['title'], item['page'], "rong360")
            self.xiunodb.db_execute(sql)

    def get_new_urls(self):
        sql = "SELECT `id`,`tid`,`bbs_tid`,`url`,`title`,`count_page`,`now_page`" \
              " FROM `bot_urls`  WHERE status =1 AND count_page > now_page;"
        return self.xiunodb.fetch_all(sql)

    def make_user_to_xiuno(self, username):
        avatar_sina = self.get_avatar()
        email = username+"@163.com"
        # 1512131334
        rand_time = random.randint(1512131334, int(time.time()))
        logins = random.randint(10, 99)
        sql = "INSERT INTO `bbs_user` (`gid`, `email`,`username`,`create_date`,`logins`, `avatar_sina`)" \
              " VALUES ('{}','{}', '{}','{}','{}','{}');"\
            .format(101, email, username, rand_time, logins, avatar_sina)
        self.xiunodb.db_execute(sql)

    def get_avatar(self):
        sql = "SELECT `avatar_sina`,`id` FROM `bot_sina_img` WHERE `status`=0 ORDER BY `id` DESC ;"
        dict = self.xiunodb.fetch_one(sql)
        sql2 = "UPDATE `bot_sina_img` SET `status`= 1 WHERE `id` = {};".format(dict['id'])
        self.xiunodb.db_execute(sql2)
        return dict['avatar_sina']

if __name__ == '__main__':
    bot = RongBot()

    post_url = "https://bbs.rong360.com/thread-522506-1.html"
    res = bot.fetch(post_url)
    if res[0]:
        html = res[1]
        posts = bot.parse_posts(html)
        for post in posts:
            print(post)

